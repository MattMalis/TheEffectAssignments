---
title: "Chapter 15 Coding Homework"
author: "Nick Huntington-Klein"
date: "Updated `r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(Statamarkdown)
```

Follow the below instructions and turn in both your code and results:

1. Create a data set with 1000 observations and four variables. Note that for A and B the values specified are the random-number-generation defaults in all languages so you shouldn't need to change anything. Also note that there are many different ways to make C and D, all are valid if they work:

- A, which is normally distributed with mean 0 and standard deviation 1
- B, which is uniformly distributed from 0 to 1
- C, which is equal to 1 (or if you prefer, True) with probability .4 and 0 (or if you prefer, False) with probability .6
- D, which is equal to 2 with probability .5, 1 with probability .3, and 0 with probability .2.

*Tip*: try tabulating the proportion of each value you get in your data for variables C and D to check if you did it properly. "Checking afterwards if you did it properly" is a very basic but far too often overlooked statistical-coding skill.

*Language-specific instructions*: 

- In Stata, you'll want to `clear` your data before doing this or else you'll get errors if you rerun your code.
- In R or Python, name the data set you create `d`. Also, only write the number 1000 (for 1000 observations) *once* in your code by assigning it to a variable outside your data set `N`, and referring to `N` instead of 1000 when generating each variable in your data set.

R: 

```{r}
library(tidyverse)

N <- 1000

d <- tibble(A = rnorm(N),
            B = runif(N),
            C = sample(0:1, N, replace = TRUE, prob = c(.4, .6)),
            D = sample(0:2, N, replace = TRUE, prob = c(.5, .3, .2)))

# Check if it worked
prop.table(table(d$C))
prop.table(table(d$D))
            
```

Stata:

```{stata}
clear

set obs 1000

g A = rnormal()
g B = runiform()
g C = runiform() < .6
g uniform_for_D = runiform()
g D = 0*(uniform_for_D < .5) + 1*(uniform_for_D >= .5 & uniform_for_D < .8) + 2*(uniform_for_D >= .8)
drop uniform_for_D

* Check if it worked
tab C
tab D
```

Python:

```{python}
import pandas as pd
import numpy as np

N = 1000

d = pd.DataFrame({
  'A': np.random.normal(size = N),
  'B': np.random.uniform(size = N),
  'C': np.random.uniform(size = N) < .6,
  'uniform_for_D': np.random.uniform(size = N)
})

d['D'] = 0*(d['uniform_for_D'] < .5) + 1*(d['uniform_for_D'] >= .5)*(d['uniform_for_D'] < .8) + 2*(d['uniform_for_D'] >= .8)
d = d.drop(labels = 'uniform_for_D', axis = 1)

# Check if we did it properly
pd.value_counts(d['C'], normalize = True)
pd.value_counts(d['D'], normalize = True)
```

2. Generate a data set with 1000 observations with the following variables:

- eps, which is normally distributed
- nu, which is normally distributed
- W, which is 0 (or False) with probability .5 and 1 (True) with probability .5
- D, our treatment variable, which is 1 if nu + (W - .5) is positive, and 0 if it's negative
- Y, our outcome variable, which is X + W + eps.

R: 

```{r}
N <- 1000

d <- tibble(eps = rnorm(N),
            nu = rnorm(N),
            W = sample(0:1, replace = TRUE)) %>% # equal probabilities are the default
  mutate(D = nu + (W-.5)>0) %>%
  mutate(Y = D + W + eps)
```

Stata:

```{stata}
clear

set obs 1000

g eps = rnormal()
g nu = rnormal()
g W = runiform() > .5
g D = nu + (W - .5) > 0
g Y = D + W + eps
```

Python:
```{python}
N = 1000

d = pd.DataFrame({
  'eps': np.random.normal(size = N),
  'nu': np.random.normal(size = N),
  'W': 1*(np.random.uniform(size = N) > .5)
})

d['D'] = 1*(d['nu'] + (d['W'] - .5) > 0)
d['Y'] = d['D'] + d['W'] + d['eps']
```

3. Write out the causal diagram paths implied by the data generating process you made in the last question (you only need to write out the direct arrows between two variables; there are five such arrows).

- D <- nu
- D <- W
- Y <- X
- Y <- W
- Y <- eps

4. Take the data-generating code from step 2. Change the way that Y is generated by adding coefficients to some of the variables. So Y is now b1X + b2W + eps. Then put it inside a function called `create_data` that lets you set N, b1, and b2 and leaves you with the created data set. In R and Python set the defaults to 1000, 1, and 0 respectively.

*Language-specific instructions*: 

- In Stata, you'll need to refer to b1 and b2 using the local-variable indicators instead of just directly. See examples in the textbook chapter. And you can set defaults if you want too, but you'll either need to use the `syntax` function for setting up your function, which we didn't cover, or do something like check if the option is specified inside of your function, and fill it in if it isn't.

R: 

```{r}
create_data <- function(N = 1000, b1 = 1, b2 = 0) {
  d <- tibble(eps = rnorm(N),
              nu = rnorm(N),
              W = sample(0:1, N, replace = TRUE)) %>% # equal probabilities are the default
    mutate(D = nu + (W-.5)>0) %>%
    mutate(Y = b1*D + b2*W + eps)
  return(d)
}
```

Stata:

```{stata}
cap prog drop create_data
prog def create_data
	local N = `1'
	local b1 = `2'
	local b2 = `3'
	
	clear
	set obs `N'

	g eps = rnormal()
	g nu = rnormal()
	g W = runiform() > .5
	g D = nu + (W - .5) > 0
	g Y = `b1'*D + `b2'*W + eps
end
```

Python:
```{python}
def create_data(N=1000, b1=1, b2=0):
  d = pd.DataFrame({
    'eps': np.random.normal(size = N),
    'nu': np.random.normal(size = N),
    'W': 1*(np.random.uniform(size = N) > .5)
  })
  
  d['D'] = 1*(d['nu'] + (d['W'] - .5) > 0)
  d['Y'] = b1*d['D'] + b2*d['W'] + d['eps']
  
  return(d)
```

5. Create a new function called `est_function` that runs `create_data` to get some data, then runs a linear regression of Y on D, and returns the coefficient on D. `est_function()` should also take N, b1, and b2 as arguments, and pass them along to `create_data`.

*Language-specific instructions*: 

- In Stata, just run the entire regression and end the function there (you don't even need the `estimates store` step if the next thing you do after calling `est_function` is get the coefficient) rather than trying to return the coefficient. This will be easier.

R: 
```{r}
est_function <- function(N = 1000, b1 = 1, b2 = 0) {
  d <- create_data(N, b1, b2)
  m <- lm(Y~D, data = d)
  return(coef(m)[2])
}
```

Stata:
```{stata}
cap prog drop est_function
prog def est_function
	local N = `1'
	local b1 = `2'
	local b2 = `3'
	
	create_data `1' `2' `3'
	
	reg Y D
end
```

Python:
```{python}
import statsmodels.formula.api as smf

def est_function(N=1000, b1=1, b2=0):
  d = create_data(N=N, b1=b1, b2=b2)
  m = smf.ols('Y~D', data = d).fit()
  return(m.params['D'])
```

6. Set your random seed to 5000. Then run `est_function` 1000 times with `N=1000, b1=1, b2=0`, storing all the results. Then show the mean and standard deviation of the estimates across the 1000 iterations. Comment on whether this estimator seems to work well. (and save your mean and SD results - we'll refer back to them later)

R:
```{r}
library(purrr)

set.seed(5000)

results <- 1:1000 %>%
  map_dbl(function(x) est_function())

mean(results)
sd(results)
```

Stata:
```{stata}
set seed 5000
clear
set obs 1000
g b1_est = .

forvalues i = 1(1)1000 {
	preserve
	quietly est_function 1000 1 0
	restore
	replace b1_est = _b[D] in `i'
}

summ b1_est
```

Python:
```{python}
np.random.seed(5000)
results = [est_function() for i in range(0,1000)]
np.mean(results)
np.std(results)
```

The average estimate is very nearly 1, which is the correct answer, so this estimator seems to work.

7. Now we're going to rerun the simulation but with b2 set to 1 instead of 0. This will help us test how well our estimator will hold up when we break one of its assumptions. Comment on what assumption we are breaking. Rerun your code from step 7 but with b2 = 1. Comment on whether the estimator performs well with this broken assumption.

R:
```{r}
set.seed(5000)

results <- 1:1000 %>%
  map_dbl(function(x) est_function(b2=1))

mean(results)
sd(results)
```

Stata:
```{stata}
set seed 5000
clear
set obs 1000
g b1_est = .

forvalues i = 1(1)1000 {
	preserve
	quietly est_function 1000 1 1
	restore
	replace b1_est = _b[D] in `i'
}

summ b1_est
```

Python:
```{python}
np.random.seed(5000)
results = [est_function(b2=1) for i in range(0,1000)]
np.mean(results)
np.std(results)
```

We are breaking the assumption that treatment D is unrelated to the error term (b2 = 1 without controlling for W puts W in the error term, and we already know that D and W are related)

We now get an estimate near 1.4, far away (and statistically significantly different from) the truth, so this makes the estimator perform poorly.

8. Create new estimation functions `est_control()` and `est_interact()`. Each of them modifies the `est_function()` function to control for W. `est_control` simply adds W as a control, so you regress Y on D and W and return the coefficient on D. `est_interact` controls for W by *fully interacting* it with D. In other words, you'll regress Y on D, W, and D times W. Then, estimate the *marginal effect at the mean* (remember Chapter 13) as (coefficient on D) + (average of W)x(coefficient on the interaction), and return that marginal effect at the mean. Set your seed to 5000 and run 1000 iterations of `est_control()` and 1000 of  `est_interact()` (both with b2 = 1). Comment on whether a fully-interacted model still works to get the right effect, and if both models work well, whether one has better precision than the other.

R:
```{r}
est_control<- function(N = 1000, b1 = 1, b2 = 1) {
  d <- create_data(N, b1, b2)
  m <- lm(Y~D+W, data = d)
  return(coef(m)[2])
}
est_interact<- function(N = 1000, b1 = 1, b2 = 1) {
  d <- create_data(N, b1, b2)
  m <- lm(Y~D*W, data = d)
  return(coef(m)[2] + mean(d$W)*coef(m)[4])
}

set.seed(5000)
results_control <- 1:1000 %>%
  map_dbl(function(x) est_control())

mean(results_control)
sd(results_control)

results_interact <- 1:1000 %>%
  map_dbl(function(x) est_interact())

mean(results_interact)
sd(results_interact)
```

Stata:
```{stata}

set seed 5000
clear
set obs 1000
g b1_control = .
g b1_interact = .

forvalues i = 1(1)1000 {
	preserve
	quietly est_control 1000 1 1
	restore
	replace b1_control = _b[D] in `i'
	
	preserve
	quietly est_interact 1000 1 1
	qui summ W
	restore
	replace b1_interact = _b[1.D] + r(mean)*_b[1.D#1.W] in `i'
}

summ b1_*
```

Python:
```{python}
def est_control(N=1000, b1=1, b2=1):
  d = create_data(N=N, b1=b1, b2=b2)
  m = smf.ols('Y~D+W', data = d).fit()
  return(m.params['D'])

def est_interact(N=1000, b1=1, b2=1):
  d = create_data(N=N, b1=b1, b2=b2)
  m = smf.ols('Y~D*W', data = d).fit()
  return(m.params['D'] + m.params['D:W']*np.mean(d['W']))

np.random.seed(5000)
results_control = [est_control() for i in range(0,1000)]
np.mean(results_control)
np.std(results_control)

results_interact = [est_interact() for i in range(0,1000)]
np.mean(results_interact)
np.std(results_interact)
```

We do get an accurate average of about 1 for both methods. There are some slight differences in standard deviation but nothing major (and if you try it with a few different seeds you might get different results).

9. Modify `est_control` to return 1 if the coefficient on D is statistically significant at the 95% level, and 0 otherwise. Then, try running it with different sample sizes (500 iterations per sample size you try) to find the minimum sample size necessary to get 90% power if b1 = .5 and b2 = .5. Report what minimum sample size is. You don't have to get it exactly - finding the smallest multiple of 100 that gives 90% power or better is close enough.

Tip: Not sure what range of sample sizes to try? Search around with a smaller number of iterations (maybe 100 instead of 500?) until you get numbers that look sort of like 90%, then increase the number of iterations to see if it holds up.

*Language-specific instructions*:

- In Stata, this will mostly require modifying your iteration loop (or iteration function, if you wrote one), rather than modifying `est_control`.

R:
```{r}
library(broom)

est_control<- function(N = 1000, b1 = 1, b2 = 1) {
  d <- create_data(N, b1, b2)
  m <- lm(Y~D+W, data = d)
  return(tidy(m)$p.value[2] <= .05)
}

iterate <- function(iter = 500, N = 1000, b1 = .5, b2 = .5) {
  1:iter %>%
    map_dbl(function(x) est_control(N, b1, b2)) %>%
    mean()
}

set.seed(5000)
c(100, 200, 300) %>%
  map_df(function(x) list(N = x, proportion_significant = iterate(N = x)))

```

Stata:
```{stata}
set seed 5000
foreach N in 100 200 300 {
	clear
	set obs 500
	g b1_sig = .

	forvalues i = 1(1)500 {
		preserve
		quietly est_control `N' .5 .5
		restore
		matrix R = r(table)
		matrix p = R["pvalue","D"]
		quietly replace b1_sig = p[1,1] <= .05 in `i'
	}
	summ b1_sig
}
```

Python:
```{python}
def est_control(N=1000, b1=1, b2=1):
  d = create_data(N=N, b1=b1, b2=b2)
  m = smf.ols('Y~D+W', data = d).fit()
  return(m.pvalues['D'] <= .05)

def iterate(iter = 500, N=500, b1 = .5, b2 = .5):
  results = [est_control(N=N, b1=b1, b2=b2) for i in range(0,iter)]
  return(np.mean(results))

np.random.seed(5000)
mss = [[N, iterate(N=N)] for N in [100, 200, 300]]
# Look for the first N with power above 90%
pd.DataFrame(mss, columns = ['N','Power'])
```

Looks like 200 observations won't quite make it to 90% power (just barely missing), but 300 will! Depending on language you might get that 200 was over the line and made it.

10. Now, keep the sample size at 500, and find the minimum detectable effect (b1) to get 90% power when b2 = .5.

R:
```{r}
set.seed(5000)
c(.1, .2, .3, .4, .5) %>%
  map_df(function(x) list(N = x, proportion_significant = iterate(b1 = x)))

```

Stata:
```{stata}
set seed 5000
foreach b1 in .1 .2 .3 .4 .5 {
	clear
	set obs 500
	g b1_sig = .

	forvalues i = 1(1)500 {
		preserve
		quietly est_control 500 `b1' .5
		restore
		matrix R = r(table)
		matrix p = R["pvalue","D"]
		quietly replace b1_sig = p[1,1] <= .05 in `i'
	}
	summ b1_sig
}
```

Python:
```{python}
np.random.seed(5000)
mde = [[b1, iterate(b1=b1)] for b1 in [.1, .2, .3, .4, .5]]
# Look for the first N with power above 90%
pd.DataFrame(mde, columns = ['b1','Power'])
```

Looks like an effect of about .3 gets us 90% power, depending on language we might find we need a bit above that.

11. Double b2 to be 1 instead of .5 and find the minimum detectable effect again. Does this change the minimum detectable effect? 

R:
```{r}
set.seed(5000)
c(.1, .2, .3, .4, .5) %>%
  map_df(function(x) list(N = x, proportion_significant = iterate(b1 = x, b2 = 1)))

```

Stata:
```{stata}
set seed 5000
foreach b1 in .1 .2 .3 .4 .5 {
	clear
	set obs 500
	g b1_sig = .

	forvalues i = 1(1)500 {
		preserve
		quietly est_control 500 `b1' 1
		restore
		matrix R = r(table)
		matrix p = R["pvalue","D"]
		quietly replace b1_sig = p[1,1] <= .05 in `i'
	}
	summ b1_sig
}
```

Python:
```{python}
np.random.seed(5000)
mde = [[b1, iterate(b1=b1, b2 = 1)] for b1 in [.1, .2, .3, .4, .5]]
# Look for the first N with power above 90%
pd.DataFrame(mde, columns = ['b1','Power'])
```

The MDE appears to be completely unchanged!